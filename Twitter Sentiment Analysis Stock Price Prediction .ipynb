{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d614bf",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Stock Price Prediction\n",
    "\n",
    "### - Crawl data\n",
    "\n",
    "### - Sentiment Analysis\n",
    "\n",
    "### - Predict Stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a07364",
   "metadata": {},
   "source": [
    "## Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a0febb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy==3.8.0\n",
      "  Downloading tweepy-3.8.0-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\thu\\anaconda3\\lib\\site-packages (from tweepy==3.8.0) (1.3.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\thu\\anaconda3\\lib\\site-packages (from tweepy==3.8.0) (1.16.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in c:\\users\\thu\\anaconda3\\lib\\site-packages (from tweepy==3.8.0) (1.7.1)\n",
      "Requirement already satisfied: requests>=2.11.1 in c:\\users\\thu\\anaconda3\\lib\\site-packages (from tweepy==3.8.0) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thu\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy==3.8.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thu\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy==3.8.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\thu\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy==3.8.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thu\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy==3.8.0) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\thu\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy==3.8.0) (3.2.0)\n",
      "Installing collected packages: tweepy\n",
      "  Attempting uninstall: tweepy\n",
      "    Found existing installation: tweepy 4.5.0\n",
      "    Uninstalling tweepy-4.5.0:\n",
      "      Successfully uninstalled tweepy-4.5.0\n",
      "Successfully installed tweepy-3.8.0\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install tweepy==3.8.0\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "import tweepy as tw\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab9a9d",
   "metadata": {},
   "source": [
    "## Set up token API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3574d54c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api_key = \"HAcIeikl6eRheQP1oKYGDwPx0\"\n",
    "api_secret = \"gEWmqG7QVXzudnviUXDo98L2UKGYg9PNqy1bAR1geClkHjqBGk\"\n",
    "access_token = \"1481095074465579008-1pApYlU4HRJWpbAXcbdVEVGzdACuEz\"\n",
    "access_token_secret = \"htgD81H4mOxXw5MglAljFRHhcfGAuwsNomXVqvGw0Obwe\"\n",
    "auth = tw.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95cdca",
   "metadata": {},
   "source": [
    "## Crawl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8b002d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apiSearch(kw, number_of_tweets, tweets, user_id, created_at):\n",
    "    for i in tw.Cursor(api.search, q = kw , lang =\"en\", since = \"2022-01-14\",tweet_mode = \"extended\").items(number_of_tweets):\n",
    "        tweets.append(i.full_text)\n",
    "        user_id.append(i.user.id)\n",
    "        created_at.append(i.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf817f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Finance\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'API' object has no attribute 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28372/2801964542.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mfinal_kw_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkw\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msince\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"2022-01-14\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"extended\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0muser_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'API' object has no attribute 'search'"
     ]
    }
   ],
   "source": [
    "number_of_tweets = 100\n",
    "tweets = []\n",
    "user_id =[]\n",
    "created_at = []\n",
    "keyword_list = ['Finance', 'Shareholders', 'Stock', 'Soaring', 'Market', 'Trends']\n",
    "company = 'Amazon'\n",
    "final_kw_list = []\n",
    "\n",
    "# Crawl data from keywords\n",
    "for i in range (len(keyword_list)):\n",
    "    kw = str(company + ' ' + keyword_list[i])\n",
    "    print(kw)\n",
    "    final_kw_list.append(kw)\n",
    "    for i in tw.Cursor(api.search, q = kw , lang =\"en\", since = \"2022-01-14\",tweet_mode = \"extended\").items(number_of_tweets):\n",
    "        tweets.append(i.full_text)\n",
    "        user_id.append(i.user.id)\n",
    "        created_at.append(i.created_at)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2feab",
   "metadata": {},
   "source": [
    "## Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Created_at\": created_at,\"User_id\": user_id,\"Tweets\":tweets})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4185b38",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16556a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.Tweets.str.contains(\"RT\")]\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9ada0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleanUpTweet(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+','',text) # Remove mentions @\n",
    "    text = re.sub(r'#', '', text) # Remove hastag symbol\n",
    "    text = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', text) # Remove hyper link\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text) # Remove special characters\n",
    "    #Remove stop words\n",
    "    res = ''\n",
    "    for word in text.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            res += WordNetLemmatizer().lemmatize(word) + ' '\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9fe596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweets'] = df['Tweets'].apply(cleanUpTweet)\n",
    "df = df.drop_duplicates(subset='Tweets')\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe43ded",
   "metadata": {},
   "source": [
    "## Create clean data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data=[]\n",
    "for tw in df['Tweets']:\n",
    "    cleaned_data.append(tw)\n",
    "    \n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4c875",
   "metadata": {},
   "source": [
    "## WordCloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62ceb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the word cloud \n",
    "allWords = ' '.join( [twts for twts in df['Tweets']] )\n",
    "positiveWordCloud = WordCloud(max_words = 1000 , width = 1600 , height = 800, collocations=False).generate(allWords)\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(positiveWordCloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f0db6",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894164a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to get subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Create function to get polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Function compute negative, positive\n",
    "def getAnalysis(score):\n",
    "    if score < 0: \n",
    "        return np.int_(0)\n",
    "    elif score > 0:\n",
    "        return np.int_(1)\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "    \n",
    "df['Subjectivity'] = df['Tweets'].apply(getSubjectivity)\n",
    "df['Polarity'] = df['Tweets'].apply(getPolarity)\n",
    "df['Sentiment'] = df['Polarity'].apply(getAnalysis)\n",
    "df.drop(df.loc[df['Sentiment']=='Neutral'].index, inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f33061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV file\n",
    "df.to_csv('Amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8df89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display 5 positive tweets\n",
    "j = 0\n",
    "sortedDF = df.sort_values(by=['Polarity'])\n",
    "positiveTweets = ''\n",
    "for i in range(sortedDF.shape[0]):\n",
    "    if (sortedDF['Sentiment'][i] == 1):\n",
    "        print(str(j) + ') ' + sortedDF['Tweets'][i])\n",
    "        positiveTweets += (' ' + sortedDF['Tweets'][i])\n",
    "        print()\n",
    "        j+=1\n",
    "    if j == 5:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1aadff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display 5 negative tweets\n",
    "j = 0\n",
    "sortedDF = df.sort_values(by=['Polarity'])\n",
    "negativeTweets = ''\n",
    "for i in range(sortedDF.shape[0]):\n",
    "    if (sortedDF['Sentiment'][i] == 0):\n",
    "        print(str(j) + ') ' + sortedDF['Tweets'][i] )\n",
    "        negativeTweets += (' ' + sortedDF['Tweets'][i])\n",
    "        print()\n",
    "        j+=1\n",
    "    if j == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot popularity and subjectivity\n",
    "plt.figure(figsize=(8,6))\n",
    "for i in range(df.shape[0]):\n",
    "    plt.scatter(df['Polarity'][i], df['Subjectivity'][i], color='Blue')\n",
    "    \n",
    "plt.title('Sentiment Analysis')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db325af",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa7336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "  \n",
    "  def clean(self, text):\n",
    "      no_html = BeautifulSoup(text).get_text()\n",
    "      clean = re.sub(\"[^a-z\\s]+\", \" \", no_html, flags=re.IGNORECASE)\n",
    "      return re.sub(\"(\\s+)\", \" \", clean)\n",
    "\n",
    " \n",
    "  def tokenize(self, text):\n",
    "      clean = self.clean(text).lower()\n",
    "      stopwords_en = stopwords.words(\"english\")\n",
    "      return [w for w in re.split(\"\\W+\", clean) if not w in stopwords_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9033c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "  \n",
    "    def __init__(self, classes, tokenizer):\n",
    "      self.tokenizer = tokenizer\n",
    "      self.classes = classes\n",
    "      \n",
    "    def group_by_class(self, X, y):\n",
    "      data = dict()\n",
    "      for c in self.classes:\n",
    "        data[c] = X[np.where(y == c)]\n",
    "      return data\n",
    "           \n",
    "    def fit(self, X, y):\n",
    "        self.n_class_items = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        n = len(X)\n",
    "        \n",
    "        grouped_data = self.group_by_class(X, y)\n",
    "        \n",
    "        for c, data in grouped_data.items():\n",
    "          self.n_class_items[c] = len(data)\n",
    "          self.log_class_priors[c] = math.log(self.n_class_items[c] / n)\n",
    "          self.word_counts[c] = defaultdict(lambda: 0)\n",
    "          \n",
    "          for text in data:\n",
    "            counts = Counter(self.tokenizer.tokenize(text))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "                \n",
    "        return self\n",
    "      \n",
    "    def laplace_smoothing(self, word, text_class):\n",
    "      num = self.word_counts[text_class][word] + 1\n",
    "      denom = self.n_class_items[text_class] + len(self.vocab)\n",
    "      return math.log(num / denom)\n",
    "      \n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for text in X:\n",
    "          \n",
    "          class_scores = {c: self.log_class_priors[c] for c in self.classes}\n",
    "\n",
    "          words = set(self.tokenizer.tokenize(text))\n",
    "          for word in words:\n",
    "              if word not in self.vocab: continue\n",
    "\n",
    "              for c in self.classes:\n",
    "                \n",
    "                log_w_given_c = self.laplace_smoothing(word, c)\n",
    "                class_scores[c] += log_w_given_c\n",
    "                \n",
    "          result.append(max(class_scores, key=class_scores.get))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Tweets'].values\n",
    "y = df['Sentiment'].values\n",
    "RANDOM_SEED = 1\n",
    "  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNaiveBayes(\n",
    "    classes=np.unique(y), \n",
    "    tokenizer=Tokenizer()\n",
    ").fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.array(MNB.predict(X_test))\n",
    "print(y_test)\n",
    "print(y_hat)\n",
    "print(type(y_hat[0]))\n",
    "print(type(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb53db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca917ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_hat)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"negative\", \"positive\"]\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False, xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Actual sentiment')\n",
    "plt.xlabel('Predicted sentiment');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
